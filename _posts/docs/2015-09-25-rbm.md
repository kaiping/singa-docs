---
layout: post
title: Example-Restricted Boltzmann Machine
subtitle: Example-Restricted Boltzmann Machine
category : docs
tags : [rbm, example]
---
{% include JB/setup %}


This example will show you how to use SINGA to run an RBM + Autoencoder model
using MNIST dataset . RBM model is a representative energy model.
In this example, we use deep autoencoder to reduce the dimensionality of
image features. RBM model is used here to initialize the weights of deep autoencoder.

## Running instructions.
We will use the MNIST dataset, which has about 60,000 handwritten digit images.
The following commands will download and convert the dataset into SINGA
recognizable format.

    cd SINGA_ROOT/examples/rbm
    cp Makefile.example Makefile
    make download
    make create

This example consists of two stages: pre-training and fine-tuning. In the pre-training stage, we define 4 RBM models. (Add a link to data shard preparation.)  For the first 3 RBM models, the activation probabilities of the hidden units will be used as the visible units of higher level RBM. In the fine-tuning stage, these 4 RBMs are “unfolded” to form encoder and decoder networks that initially use the same weights.

•	Pre-training Stage
Some important fields of the RBM Layer (Add a link to layer desrciption) configuration file are shown below. The algorithm we choose is contrastive divergence (kCD). (Add a link to Contrastive Divergence.)
•	For updater, we use momentum and weight decay, SGD algorithm with fixed learning rate. (Add a link to updater)
•	The 4 RBM models have the same cluster configuration. (Add a link to cluster configuration.)
•	The workspace defines the path where this model’s checkpoint is stored and checkpoint_path defines the path where checkpoint is loaded. (Add a link to checkpoint.)
•	For RBM model, the weight matrix is shared by,RBMVis and RBMHid layers. We use “share_from” to denote the shared matrix.


Configuration for RBM Updater


Configuration for RBM cluster

Configuration for RBM Layer

Configuration for linear RBM Layer


Configuration for linear RBM Updater

o	Step1: Configure a 784X1000 RBM model (RBM0, Figure 1) and save its checkpoint to "/data/zhaojing/checkpoint/rbm0/" by configuring workspace.
o	Step2: Configure a 1000X500 RBM model (RBM1, Figure 2) and load the checkpoint of RBM0 from "/data/zhaojing/checkpoint/rbm0/checkpoint/step6000-worker0.bin" by configuring checkpoint_path. Also it should save its checkpoint to "/data/zhaojing/checkpoint/rbm1/" by configuring workspace.
o	Step3: Configure a 500X250 RBM model (RBM2, Figure 3) and load the checkpoint of RBM1 from "/data/zhaojing/checkpoint/rbm1/checkpoint/step6000-worker0.bin" by configuring checkpoint. Also it should save its checkpoint to "/data/zhaojing/checkpoint/rbm2/" by configuring.
o	Step4 : Configure a 250X30 RBM model (RBM3, Figure 4) and load the checkpoint of RBM2 from "/data/zhaojing/checkpoint/rbm2/checkpoint/step6000-worker0.bin" by configuring checkpoint. Also it should save its checkpoint to "/data/zhaojing/checkpoint/rbm3/" by configuring workspace. But for RBM3 model, it is a linear RBM, so we add a “gaussian” filed in the RBMHid layer and it has a much smaller learning rate.

Figure 1


Figure 2

Figure 3

Figure 4

Figure 5
•	Fine-Tuning Stage
Some important fields of the autoencoder configuration file are shown below. The algorithm we choose is back propagation (kBP). (Add a link to Back Propagation.) We use the same cluster configuration as RBM models. For updater, we use AdaGradient algorithm with fixed learning rate.
o	Step1 : Configure an auto encoder with 7 hidden layers (Figure 5). Load the checkpoints of previous 4 RBM models and save its checkpoint to "/data/zhaojing/checkpoint/rbm0/" by configuring workspace

 Configuration file for autoencoder updater and checkpoint_path

•	Visualization Results
Figure (a) visualizes sample columns of the weight matrix of RBM0, We can see the Gabor-like filters are learned. Figure (b) depicts the features extracted from the top-layer of the auto-encoder, wherein one point represents one image. Different colors represent different digits. We can see that most images are well clustered according to the ground truth.

(a)	Bottom RBM weight matrix



(b)	Top layer features
References
[1] http://www.cs.toronto.edu/~hinton/science.pdf
[2] http://yann.lecun.com/exdb/mnist/




