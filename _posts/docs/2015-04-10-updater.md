---
layout: post
title:  Updater
category : docs
tags : [updater]
---
{% include JB/setup %}

Every server in SINGA has a Updater instance which minus each parameter with a
delta value computed based on its gradient sent from workers.

## Configuration
There are many
different updaters (subclasses of Updater) for computing the delta value. They share some
configuration fields like

* type, an integer for identifying an updater; User-defined updater should have
 different type ID with built-in updaters.
* base_lr, initial learning rate
* lr_change, an integer for identifying the learning rate change method.
* weight_decay, the co-efficient for [L2 regularization](http://deeplearning.net/tutorial/gettingstarted.html#regularization)
* [momentum](http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/)

If you are not familiar with the above terms, you can get their meanings in
[this page by Karpathy](http://cs231n.github.io/neural-networks-3/#update).

Some udpaters may have specific configurations. E.g., the RMSprop updater
should be configured like

    updater {
      type: kRMSProp
      rmsprop_conf {
       ...
      }
    }

The learning rate change methods also have specific configuration fields. E.g.,
one learning rate change method is of type kFixedStep, it should be configured
as

    updater {
      ...
      fixedstep_conf {
       ...
      }
    }

## Base Updater class
The base Updater class has one virtual function,

    class Updater{
     public:
      virtual void Update(int step, Param* param, float grad_scale = 1.0f) = 0;

     protected:
      UpdaterProto proto_;
    };

It updates the values of the `param` based on its gradients. The `step`
argument is for deciding the learning rate which may change through time
(step). `grad_scale` scales the original gradient values. This function is
called by servers once it receives all gradients for the same Param object.
The updater configuration is passed to `proto_` in an `Init` function.

## Implementing a new Updater subclass

To implement a new Updater subclass, users must override the `Update` function.

    class FooUpdater : public Updater {
      void Update(int step, Param* param, float grad_scale = 1.0f) override;
    };

Configuration of this new updater can be declared similar to that of a new
layer,

    # in user.proto
    FooUpdaterProto {
      optional int32 c = 1;
    }

    extend UpdaterProto {
      optional FooUpdaterProto fooupdater_conf= 101;
    }

    # in job.proto
    UpdaterProto {
      ...
      extension 101..max;
    }

The new updater should be registered in the main.cc,

    const int kFooUpdater =10; // ID of the udpater, should be different with built-in udpaters
    driver.RegisterUpdater<FooUpdater>(kFooUpdater);

Users can then configure the job as

    # in job.conf
    updater {
      type: 10
      fooupdater_conf {
        c : 20;
      }
    }

## Built-in Updater subclasses

### Normal SGD

To use the normal SGD, users need to provide at least the `base_lr`.
`momentum` and `weight_decay` are optional fields.

### AdaGradUpdater

AdaGrad is configured similar to normal SGD except that the `momentum` is not
ued.

### NesterovUpdater

Nesterov must be configured with `base_lr` and `momentum`. `weight_decay` is an
optional configuration field.

### RMSPropUpdater

### AdaDeltaUpdater

## Built-in learning rate change methods

### kFixed

The `base_lr` is used for all steps.

### kLinear

The updater should be configured like

    updater {
      base_lr:  float
      linear_conf {
        freq: int
        final_lr: float
      }
    }

Linear interpolation is used to change the learning rate,

    lr = (1 - step / freq) * base_lr + (step / freq) * final_lr

### kExponential

The udpter should be configured like

    updater {
      base_lr: float
      exponential_conf {
        freq: int
      }
    }

The learning rate for `step` is

    lr = base_lr / 2^(step / freq)

### kInverseT

The updater should be configured like

    updater {
      base_lr: float
      inverset_conf {
        final_lr: float
      }
    }

The learning rate for `step` is

    lr = base_lr / (1 + step / final_lr)

### kInverse

The updater should be configured like

    updater {
      base_lr: float
      inverse_conf {
        gamma: float
        pow: float
      }
    }


The learning rate for `step` is

    lr = base_lr * (1 + gamma * setp)^(-pow)


### kStep

The updater should be configured like

    updater {
      base_lr : float
      step_conf {
        change_freq: int
        gamma: float
      }
    }


The learning rate for `step` is

    lr = base_lr * gamma^ (step / change_freq)

### kFixedStep

The updater should be configured like

    updater {
      fixedstep_conf {
        step: int
        step_lr: float

        step: int
        step_lr: float

        ...
      }
    }

Denote the i-th tuple as (step[i], step_lr[i]), then the learning rate for
`step` is,

    step_lr[k]

where step[k] is the smallest number that is larger than `step`.
