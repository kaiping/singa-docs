---
layout: post
title: Example --- Recurrent Neural Network
category : docs
tags : [rnn, example]
---
{% include JB/setup %}


Recurrent Neural Networks (RNN) are widely used for modeling sequential data,
such as music, videos and sentences.  In this example, we use SINGA to train a
[RNN model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
proposed by Tomas Mikolov for [language modeling](https://en.wikipedia.org/wiki/Language_model).
The training objective (loss) is to
minimize the [perplexity per word](https://en.wikipedia.org/wiki/Perplexity), which
is equivalent to maximize the probability of predicting the next word given the current word in
a sentence.

Different from the [CNN]({{ BASE_PATH }}/docs/cnn), [MLP]({{ BASE_PATH }}/docs/mlp)
and [RBM]({{ BASE_PATH }}/docs/rbm) examples which use built-in
[Layer]({{ BASE_PATH }}/docs/layer)s and [Record]({{ BASE_PATH }}/docs/data)s,
none of the layers in this model is built-in. Hence users can get examples of
implementing their own Layers and data Records in this page.

## Running instructions

In *SINGA_ROOT/examples/rnn/*, scripts are provided to run the training job.
First, the data is prepared by

    $ cp Makefile.example Makefile
    $ make download
    $ make create

Second, the training is started by passing the job configuration as,

    # in SINGA_ROOT
    $ ./bin/singa-run.sh -conf SINGA_ROOT/examples/rnn/job.conf



## Implementations

<img src="{{ BASE_PATH }}/assets/image/rnn-refine.png" align="center" width="300px"/>
<span><strong>Figure 1 - Net structure of the RNN model.</strong></span>

The neural net structure is shown in Figure 1.
Word records are loaded by `RnnlmDataLayer` from `WordShard`. `RnnlmWordparserLayer`
parses word records to get word indexes (in the vocabulary). For every iteration,
`window_size` words are processed. `RnnlmWordinputLayer` looks up a word
embedding matrix to extract feature vectors for words in the window.
These features are transformed by `RnnlmInnerproductLayer` layer and `RnnlmSigmoidLayer`.
`RnnlmSigmoidLayer` is a recurrent layer that forwards features from previous words
to next words.  Finally, `RnnlmComputationLayer` computes the perplexity loss with
word class information from `RnnlmClassparserLayer`. The word class is a cluster ID.
Words are clustered based on their frequency in the dataset, e.g., frequent words
are clustered together and less frequent words are clustered together. Clustering
is to improve the efficiency of the final prediction process.

### Data preparation

We use a small dataset in this example. To be specific, we use a training dataset with 10000 sentences, testing dataset with 1000 sentences and validation dataset with 999 sentences. All punctuation characters except for `'` are ignored.
The subsequent steps follow the instructions in
[Data Preparation]({{ BASE_PATH }}/docs/data) to convert the
raw data into `Record`s and insert them into `DataShard`s.

#### Download source data

    # in SINGA_ROOT/examples/rnn/
    wget https://f25ea9ccb7d3346ce6891573d543960492b92c30.googledrive.com/host/0ByxdPXuxLPS5RFM5dVNvWVhTd0U/rnnlm-0.4b.tgz
    
Note: This source data file can be found in [RNN Toolkit](http://rnnlm.org/) implemented by Tomas Mikolov.

#### Define your own record.

Since this dataset has a different format from the built-in `SingleLabelImageRecord`,
we need to extend the base `Record` to add new fields,

    # in SINGA_ROOT/examples/rnn/user.proto
    package singa;

    import "common.proto";  // import SINGA Record

    extend Record {  // extend base Record to include users' records
        optional WordClassRecord wordclass = 101;
        optional SingleWordRecord singleword = 102;
    }

    message WordClassRecord {
        optional int32 class_index = 1; // the index of this class
        optional int32 start = 2; // the index of the start word in this class;
        optional int32 end = 3; // the index of the end word in this class
    }

    message SingleWordRecord {
        optional string word = 1;
        optional int32 word_index = 2;   // the index of this word in the vocabulary
        optional int32 class_index = 3;   // the index of the class corresponding to this word
    }


#### Create data shard for training and testing

{% comment %}
As the vocabulary size is very large, the original perplexity calculation method
is time consuming. Because it has to calculate the probabilities of all possible
words for

    p(wt|w0, w1, ... wt-1).


Tomas proposed to divide all
words into different classes according to the word frequency, and compute the
perplexity according to

    p(wt|w0, w1, ... wt-1) = p(c|w0,w1,..wt-1) p(wt|c)

where `c` is the word class for `wt`, `w0, w1...wt-1` are the previous words before `wt`.
The probabilities on the right side can be computed faster than the left side.


[Makefile](https://github.com/kaiping/incubator-singa/blob/rnnlm/examples/rnnlm/Makefile)
for creating the shards (see in
  [create_shard.cc](https://github.com/kaiping/incubator-singa/blob/rnnlm/examples/rnnlm/create_shard.cc)),
  we need to specify where to download the source data, number of classes we
  want to divide all occurring words into, and all the shards together with
  their names, directories we want to create.
{% endcomment %}

*SINGA_ROOT/examples/rnn/create_shard.cc* defines the following function for creating data shards respectively for training, testing and validation,

    void create_shard(const char *input, int nclass) {

`input` is the path to the train, test, and validation text files, `nclass` is user specified cluster size.
This function starts with

      using StrIntMap = std::map<std::string, int>;
      StrIntMap *wordIdxMapPtr;	//	Mapping word string to a word index
      StrIntMap *wordClassIdxMapPtr;	//	Mapping word string to a word class index
      if (-1 == nclass) {
          loadClusterForNonTrainMode(&wordIdxMap, &wordClassIdxMap); // non-training phase
      } else {
          doClusterForTrainMode(input, nclass, &wordIdxMap, &wordClassIdxMap); // training phase
      }


  * If `nclass` is not equal to `-1`, `path` points to the training data file.  `doClusterForTrainMode`
  reads all the words in the file, divide these words into `nclass`clusters, create a vocabulary shard as well as a class shard, and meanwhile create two maps. The two maps respectively store the mapping information between word strings and word indices, the mapping information between word strings and class indices.
  * Otherwise, `path` points to either test or validation data file. `loadClusterForNonTrainMode`
  loads the information from the vocabulary shard and the class shard, to create the two maps.
  * In both modes, with the mapping information in the two maps, input words in text files are transformed into `Records` in desired shards. 

Words from training/text/validation files are converted into `Record`s by

      singa::SingleWordRecord *wordRecord = record.MutableExtension(singa::singleword);
      while (in >> word) {
        wordRecord->set_word(word);
        wordRecord->set_word_index(wordIdxMap[word]);
        wordRecord->set_class_index(wordClassIdxMap[word]);
        snprintf(key, kMaxKeyLength, "%08d", wordIdxMap[word]);
        wordShard.Insert(std::string(key), record);
      }
    }

Compilation and running commands are provided in the *Makefile.example*.
After executing

    make create

three data shards will be created using the `create_shard.cc`, namely,
*rnnlm_word_shard_train*, *rnnlm_word_shard_test* and *rnnlm_word_shard_valid*, respectively used in training, testing and validation.


### Layer implementation

7 layers (i.e., Layer subclasses) are implemented for this application,
including 1 [data layer]({{ BASE_PATH }}/docs/layer#data-layers) which fetches data records from data
shards, 2 [parser layers]({{ BASE_PATH }}/docs/layer#parser-layers) which parses the input records, 3 neuron layers
which transforms the word features and 1 loss layer which computes the
objective loss.

First, we briefly illustrate the role in this language model application for each layer. Then, we
discuss the layers' configuration and functionality. Finally, we introduce how
to configure a job and then run the training for your own model.

Following the guide for implementing [new Layer subclasses]({{ BASE_PATH }}/docs/layer#implementing-a-new-layer-subclass),
we extend the [LayerProto]({{ BASE_PATH }}/api/classsinga_1_1LayerProto.html)
to include the configuration message of each user-defined layer as shown below
(5 out of the 7 layers have specific configurations),

    package singa;

    import "common.proto";  // Record message for SINGA is defined
    import "job.proto";     // Layer message for SINGA is defined

    //For implementation of RNNLM application
    extend LayerProto {
        optional RnnlmComputationProto rnnlmcomputation_conf = 201;
        optional RnnlmSigmoidProto rnnlmsigmoid_conf = 202;
        optional RnnlmInnerproductProto rnnlminnerproduct_conf = 203;
        optional RnnlmWordinputProto rnnlmwordinput_conf = 204;
        optional RnnlmDataProto rnnlmdata_conf = 207;
    }


In the subsequent sections, we describe the implementation of each layer, including
its configuration message as well as its functionality .

### RnnlmDataLayer

It inherits [DataLayer]({{ BASE_PATH }}/api/classsinga_1_1DataLayer.html) for loading word and
class `Record`s from `DataShard`s into memory.

#### Functionality

    void RnnlmDataLayer::Setup() {
      records_.resize(windowsize_ + 1);
      classsize_ = classshard_->Count();  // Obtain class_size
      singa::WordClassRecord wcr;
      for (int i = 0; i < classsize_; i++) {
        classshard_->Next(&class_key, &sample_);
        wcr = sample_.GetExtension(wordclass);
      }
       wordshard_->Next(&word_key, &records_[windowsize_]);
    }


    void RnnlmDataLayer::ComputeFeature() {
      records_[0] = records_[windowsize_];	//Copy the last record to 1st position
      for (int i = 1; i < records_.size(); i++) {  //Read window_size new word records from WordShard
        string key;
        wordshard_->Next(&key, &records_[i]);
      }
    }


The `Setup` function load the mapping (from word string to class index) from
*ClassShard*.

Every time the `ComputeFeature` function is called, it loads `windowsize_` number of `SingleWordRecord`s
from `WordShard` and these records are stored as a member `records_` in this layer.



#### Configuration

    message RnnlmDataProto {
        required string class_path = 1;   // path to the class data folder, absolute or relative to the workspace
        required string word_path = 2;    // path to the word data folder, absolute or relative to the workspace
        required int32 window_size = 3;   // window size.
    }
    
The `class_path` in configuration is the path to the class shard, storing all information of word classes. The `word_path` is the path to the word shard generated using the text file (train, test or validation). Besides, the `window_size` is set to the number of words to process each time.

### RnnlmWordParserLayer

This layer gets `window_size` word records from the `RnnlmDataLayer` and store `window_size` corresponding word indices as the `data_` field in this layer.

#### Functionality

    void RnnlmWordparserLayer::Setup(){
      windowsize_ = static_cast<RnnlmDataLayer*>(srclayers_[0])->windowsize();
      vocabsize_ = static_cast<RnnlmDataLayer*>(srclayers_[0])->vocabsize();
      data_.Reshape(vector<int>{windowsize_});
    }

    void RnnlmWordparserLayer::ParseRecords(Blob* blob){
    //for each word record in the window, get its word index and insert into blob
      for (int i = 0; i < records.size() - 1; i++) {
      blob[i] = records[i].GetExtension(singleword).word_index();
      }
    }

The `Setup` function reads the `window_size` and `vocab_size` from its source layer and resize its `data_` field. Then each time `ParseRecords` is called, all the word records. indices are inserted into `blob` and then passed upward to `RnnlmWordInputLayer`.
#### Configuration

This layer does not have specific configuration fields.


### RnnlmClassParserLayer

This layer reads all class records from `RnnlmDataLayer` and stores the class information. Besides, it obtains the information specific to the next word of each word in the processing window and passes upward to `RnnlmComputationLayer`.

#### Functionality

    void RnnlmClassparserLayer::Setup(){
      windowsize_ = static_cast<RnnlmDataLayer*>(srclayers_[0])->windowsize();
      vocabsize_ = static_cast<RnnlmDataLayer*>(srclayers_[0])->vocabsize();
      classsize_ = static_cast<RnnlmDataLayer*>(srclayers_[0])->classsize();
      data_.Reshape(vector<int>{windowsize_, 4});
    }

    void RnnlmClassparserLayer::ParseRecords(){
      for(int i = 1; i < records.size(); i++){
        swr = records[i].GetExtension(singleword);
        int word_idx = swr.word_index();
        int class_idx = swr.class_index();
        data_[i] <- (start, end, word_idx, class_idx) from class_info;
      }
    }

This layer fetches the class information (the mapping information between classes and words) from `RnnlmDataLayer` and maintains this information in this layer.

The `Setup` function obtains the `windowsize_`, `vocabsize_` and `classsize_` from its source layer and then reshapes its `data_` field. 

Next, In the `ParseRecords`, this layer parses the last `window_size` number of word records from `RnnlmDataLayer`. Then, it retrieves the corresponding class for the next word of each input word in the processing window. It obtains the starting word index of this class,
ending word index of this class, word index and class index respectively and stores all these four terms as its `data_` field.

#### Configuration

This layer does not have specific configuration fields.


### RnnlmWordInputLayer

Using the input word indices, this layer inserts corresponding word vectors into
its `data_`. Then, it passes the `data_` to `RnnlmInnerProductLayer` above for further
processing.

#### Functionality
In setup phase, this layer first reshapes its members such as `data_`, `grad_`,
and `weight_` matrix. Then, it obtains the `vocabsize_` from its source layer
(i.e., `RnnlmWordParserLayer`).

    void RnnlmWordinputLayer::Setup() {
      const auto& src = srclayers_[0]->data(this);
      windowsize_ = src.shape()[0];
      vdim_ = src.count()/windowsize_;
      hdim_ = proto.GetExtension(rnnlmwordinput_conf).word_length();
      setup data_, grad_ with {windowsize_, hdim_};
      vocabsize_ = RnnlmWordparserLayer->vocabsize();
      setup weight_ with {vocabsize_, hdim_};
    }

In the forward phase, using the `window_size` number of input word indices, the
`window_size` number of word vectors are selected from this layer's `weight_`
matrix, each word index corresponding to one row.

    void RnnlmWordinputLayer::ComputeFeature() {
      for(int t = 0; t < windowsize_; t++){
        // src[t] = word index
        data[t] = weight[src[t]];
      }
    }

In the backward phase, after computing this layer's gradient in its destination
layer (i.e., `RnnlmInnerProductLayer`), here the gradient of the weight matrix in
this layer is copied (by row corresponding to word indices) from this layer's
`grad_` member.

    void RnnlmWordinputLayer::ComputeGradient() {
      for(int t = 0; t < windowsize_; t++){
        // src[t] = word index
        gweight[src[t]] = grad[t];
      }
    }


#### Configuration
In this layer, the length of each word vector is configured. Besides,
whether to use bias term during the training process should also be configured
(See more in [job.proto](https://github.com/kaiping/incubator-singa/blob/rnnlm/src/proto/job.proto)).

    message RnnlmWordinputProto {
        required int32 word_length = 1;
        optional bool bias_term = 30 [default = true];
    }


### RnnlmInnerProductLayer

This is a neuron layer which receives the data (one vector for each word) from `RnnlmWordInputLayer` and
sends the computation results to `RnnlmSigmoidLayer`.

#### Functionality
In `Setup`, this layer first obtains `window_size` and then reshapes its members such as `data_`, `grad_`, and `weight_` matrix. 

    void RnnlmInnerproductLayer::Setup() {
      const auto& src = srclayers_[0]->data(this);
      windowsize_ = src.shape()[0];
      vdim_ = src.count()/windowsize_;  // word_length
      hdim_ = proto.GetExtension(rnnlminnerproduct_conf).num_output();
      // reshape members
      setup data_, grad_ with vector<int>{windowsize_, hdim_};
      weight_->Setup(proto.param(0), vector<int>{vdim_, hdim_});
    }

In the forward phase, this layer computes the dot multiplication between its weight matrix and the `data_` in its source layer (i.e., `RnnlmWordInputLayer`). Then the product is stored as `RnnlmInnerproductLayer`'s `data_`.

    void RnnlmInnerproductLayer::ComputeFeature() {
        data = dot(src, weight);
    }

In the backward phase, this layer needs to first compute the gradient of its
source layer (i.e., `RnnlmWordInputLayer`). Then, it needs to compute the
gradient of its weight matrix by aggregating computation results for each
timestamp. The details can be seen as follows.

    void RnnlmInnerproductLayer::ComputeGradient() {
      auto grad = Tensor2(&grad_);
      auto weight = Tensor2(weight_->mutable_data());
      auto gweight = Tensor2(weight_->mutable_grad());
      Initialize gweight using 0;
      for (int t = 0; t < windowsize_; t++) {
        gweight += dot(src[t].T(), grad[t]);
      }
      gsrc = dot(grad, weight.T());
    }

#### Configuration
In this layer, the number of neurons is specified. Besides, whether to use a bias term should also be configured.

    message RnnlmInnerproductProto {
        required int32 num_output = 1;
        optional bool bias_term = 30 [default = true];
    }
    
### RnnlmSigmoidLayer

This is a neuron layer for computation which uses a previous timestamp's data as part of the input. This is how the time-order information is utilized in this language model application.

Besides, if you want to implement a recurrent neural network following our design, this layer is of vital importance for you to refer to. Also, you can always think of other design methods to make use of information from past timestamps.


#### Functionality

In `Setup`, this layer first obtains `window_size` and then reshapes its members such as `data_`, `grad_`, and `weight_` matrix. 

    void RnnlmSigmoidLayer::Setup() {
      const auto& innerproductData = srclayers_[0]->data(this);
      windowsize_ = innerproductData.shape()[0];
      vdim_ = innerproductData.count()/windowsize_;
      hdim_ = vdim_;
      // reshape members
      reshape data_, grad_ like source layer's data_ and grad_;
      // Weight matrix between s(t-1) and s(t)
      weight_->Setup(proto.param(0), vector<int>{vdim_, hdim_});
    }


In the forward phase, this layer first receives data from its source layer
(i.e., `RnnlmInnerProductLayer`) which is used as one part of input for computation.
Then, for each timestamp this layer computes the dot product between its
previous timestamp information and its own weight matrix. The results are the
other part for computation. This layer sums these two parts together and
executes an activation operation. The detailed descriptions for this process
are illustrated as follows.

    void RnnlmSigmoidLayer::ComputeFeature() {
      auto data = Tensor2(&data_);
      auto src = Tensor2(srclayers_[0]->mutable_data(this));
      auto weight = Tensor2(weight_->mutable_data());
      for(int t = 0; t < window_size; t++){
        if (t == 0) {
          data[t] = F<op::sigmoid>(src[t]);
        } else {
          data[t] = dot(data[t - 1], weight);
          data[t] += src[t];
          data[t] = F<op::sigmoid>(data[t]);
        }
    }

In the backward phase, this `RnnlmSigmoidLayer` first updates this layer's member
`grad_` utilizing the information from the next timestamp. Then this layer computes the gradient for its weight matrix and the gradient for its source layer `RnnlmInnerProductLayer` by iterating different timestamps. The detailed process can be seen below.

    void RnnlmSigmoidLayer::ComputeGradient(){
      auto data = Tensor2(&data_);
      auto grad = Tensor2(&grad_);
      auto weight = Tensor2(weight_->mutable_data());
      auto gweight = Tensor2(weight_->mutable_grad());
      // update grad[t] by adding a new term from next timestamp
      grad[t] += F<op::sigmoid_grad>(data[t+1]) * (dot(grad[t + 1], weight));
      // compute gradients for weight matrix and source layer
      for (int t = 0; t < windowsize_; t++) {
        if (t == 0) {
          gsrc[t] = F<op::sigmoid_grad>(data[t]) * grad[t];
        } else {
          gweight += F<op::sigmoid_grad>(data[t]) * (dot(data_[t-1], grad_[t].T()));
          gsrc[t] = F<op::sigmoid_grad>(data[t]) * grad[t];
        }
    }


#### Configuration

In this layer, whether to use a bias term is specified.

    message RnnlmSigmoidProto {
        optional bool bias_term = 1 [default = true];
    }


### RnnlmComputationLayer

This layer is a loss layer which computes perplexity as the performance metric. 

To be specific, this layer is composed of the class information part and the word information part. Therefore, the computation can be essentially divided into two parts by slicing this layer's weight matrix.

#### Functionality
In `Setup`, this layer first obtains `window_size`, `class_size`, `vocab_size` from `RnnlmClassparserLayer`, and then reshapes its members such as `data_`, `grad_`, and `weight_` matrix. 

    void RnnlmComputationLayer::Setup() {
      const auto& sigmoidData = srclayers_[0]->data(this);
      windowsize_ = sigmoidData.shape()[0];
      vdim_ = sigmoidData.count()/windowsize_;
      classsize_ = RnnlmClassparserLayer->classsize();
      vocabsize_ = RnnlmClassparserLayer->vocabsize();
      hdim_ = classsize_ + vocabsize_;
      // reshape members
      setup data_, grad_ with vector<int>{windowsize_, hdim_};
      weight_->Setup(proto.param(0), vector<int>{hdim_, vdim_});
    }

In the forward phase, by using the two sliced weight matrices (one is for class
information, the other is for the words in this class), this
`RnnlmComputationLayer` calculates the dot product between the source layer's
input and the sliced matrices. The results can be denoted as `y1` and `y2`.
Then after a softmax function, for each input word, the probability
distribution of classes and the words in this classes are computed. The
activated results can be denoted as `p1` and `p2`. Next, using the probability
distribution, the PPL value is computed.

    void RnnlmComputationLayer::ComputeFeature() {
      auto data = Tensor2(&data_);
      auto sigmoidData = Tensor2(srclayers_[0]->mutable_data(this));
      const float * label = srclayers_[1]->data(this).cpu_data();
      auto weight = Tensor2(weight_->mutable_data());
      auto weightPart1 = weight.Slice(0, classsize_);
      auto weightPart2 = weight.Slice(classsize_, classsize_ + vocabsize_);
      float sum = 0.0;
      float ppl = 0.0;
      
      // compute y1 and y2;
      for (int t = 0; t < windowsize_; t++) {
        int startVocabIndex = label[t][0];
        int endVocabIndex = label[t][1];
        auto weightPart2Slice = weightPart2.Slice(startVocabIndex, endVocabIndex + 1); 
        y1 = dot(sigmoidData[t], weightPart1.T());
        y2 = dot(sigmoidData[t], weightPart2Slice.T());
      }

      // compute p1 and p2;
      for (int t = 0; t < windowsize_; t++) {
        int startVocabIndex = label[t][0];
        int endVocabIndex = label[t][1];
        int wordIndex = label[t][2]);
        int classIndex = label[t][3];
        p1 = Softmax(y1);
        p2 = Softmax(y2);
        data[t]'s class part <- p1;
        data[t]'s word part for the class <- p2;
        // compute sum;
        sum += log(p1[classIndex] * p2[wordIndex - startVocabIndex]);
      }
        // compute PPL value
        ppl = exp(-(1.0 / windowsize_) * sum);  // Per word
    }


In the backward phase, this layer executes the following three computation
operations. First, it computes the member `grad_` of the current layer by each
timestamp. Second, this layer computes the gradient of its own weight matrix `gweight` by
aggregating calculated results from all timestamps. Third, it computes the
gradient `gsrc_` of its source layer, i.e., `RnnlmSigmoidLayer`, timestamp-wise.

    void RnnlmComputationLayer::ComputeGradient(){
      const float * label = srclayers_[1]->data(this).cpu_data();
      auto gweight = Tensor2(weight_->mutable_grad());
      auto gweightPart1 = gweight.Slice(0, classsize_);
      auto gweightPart2 = gweight.Slice(classsize_, classsize_ + vocabsize_);
      auto weight = Tensor2(weight_->mutable_data());
      auto weightPart1 = weight.Slice(0, classsize_);
      auto weightPart2 = weight.Slice(classsize_, classsize_ + vocabsize_);
      
      for (int t = 0; t < windowsize_; t++) {
        int start = label[t][0]);
        int end = label[t][1]);
        int wordIndex = label[t][2]);
        int classIndex = label[t][3]);
        auto gweightPart2Slice = gweightPart2.Slice(start, end + 1);
        auto weightPart2Slice = weightPart2.Slice(start, end + 1);
        // compute grad[t] for all timestamps;
        for (int i = 0; i < classsize_; i++) {
          grad[t][i] = data[t][i];
        }
        grad[t][classIndex] = data[t][classIndex] - 1;

        for (int j = classsize_; j < classsize_ + vocabsize_; j++) {
          if (j in [classsize_ + start, classsize_ + end]) {
            grad[t][j] = data[t][j];
          } else {
            grad[t][j] = 0;
          }
          grad[t][classsize_ + wordIndex] = data[t][classsize_ + wordIndex] - 1;
        }

        // compute gweight by aggregating results computed in different timestamps;
        Set gradPart1 as grad[t]'s class part;
        gweightPart1 += dot(gradPart1, src[t]);  // Aggregate updates
        Set gradPart2Slice as grad[t]'s word part for the class;
        gweightPart2Slice += dot(gradPart2Slice, src[t]);
        //Compute gsrc[t] for all timestamps;
        gsrc[t] = dot(gradPart1, weightPart1);
        gsrc[t] += dot(gradPart2Slice, weightPart2Slice);
      }
    }

#### Configuration

In this layer, it is needed to specify whether to use a bias term during training.

    message RnnlmComputationProto {
        optional bool bias_term = 1 [default = true];  // use bias vector or not
    }


## Updater Configuration

We employ kFixedStep type of the learning rate change method and the
configuration is as follows. We use different learning rate values in different
step ranges. [Here](http://wangwei-pc.d1.comp.nus.edu.sg:4000/docs/updater/) is
more information about choosing updaters.

    updater{
        #weight_decay:0.0000001
        lr_change: kFixedStep
        type: kSGD
        fixedstep_conf:{
          step:0
          step:42810
          step:49945
          step:57080
          step:64215
          step_lr:0.1
          step_lr:0.05
          step_lr:0.025
          step_lr:0.0125
          step_lr:0.00625
        }
    }


## TrainOneBatch() Function

We use BP (BackPropagation) algorithm to train the RNN model here. The
corresponding configuration can be seen below.

    # In job.conf file
    alg: kBackPropagation

Refer to
[here](http://wangwei-pc.d1.comp.nus.edu.sg:4000/docs/train-one-batch/) for
more information on different TrainOneBatch() functions.

## Cluster Configuration

In this RNN language model, we configure the cluster topology as follows.

    cluster {
      nworker_groups: 1
      nserver_groups: 1
      nservers_per_group: 1
      nworkers_per_group: 1
      nservers_per_procs: 1
      nworkers_per_procs: 1
      workspace: "examples/rnnlm/"
    }

This is to train the model in one node. For other configuration choices, please
refer to [here](http://wangwei-pc.d1.comp.nus.edu.sg:4000/docs/frameworks/).


## Configure Job

Job configuration is written in "job.conf".

Note: Extended field names should be embraced with square-parenthesis [], e.g., [singa.rnnlmdata_conf].


## Run Training

Start training by the following commands

    cd SINGA_ROOT
    ./bin/singa-run.sh -workspace=examples/rnnlm

