---
layout: post
title: Layer
category : docs
tags : [layer ]
---
{% include JB/setup %}
Layer is a core abstraction in SINGA. It performs a variety of feature
transformations for extracting high-level features, e.g., loading raw features,
parsing RGB values, doing convolution transformation, etc.

This page explains how to implement or setup
  * [Configurations of layers](#config)
  * [Base layer class](#baselayer)
  * [Built-in layer subclasses](#builtin)
  * [User-defined layer subclasses](#newlayer)

<a name ="config">
## Configuration

Configurations of layers should be written in job.conf. As examples, we show three layers from the [MLP example]({{ BASE_PATH }}/docs/mlp) below.

    layer {
      name: "data"
      type: kShardData
      sharddata_conf { }
      exclude: kTest
      partition_dim : 0
    }
    layer{
      name: "mnist"
      type: kMnist
      srclayers: "data"
      mnist_conf { }
    }
    layer{
      name: "fc1"
      type: kInnerProduct
      srclayers: "mnist"
      innerproduct_conf{ }
      param{ }
      param{ }
    }

There are some common fields for all kinds of layers:

  * `name`: a string used to differentiate two layers.
  * `type`: an integer used for identifying a Layer subclass. The types of built-in
  layers are listed in LayerType (defined in job.proto).
  For user-defined layer subclasses, `user_type` of string should be used instead of `type`.
  The detail is explained in the [last section](#newlayer) of this page.
  * `srclayers`: one or more layer names, for identifying the source layers.
  In SINGA, all connections are [converted]({{ BASE_PATH }}/docs/neural-net) to directed connections.
  * `exclude`: an enumerate value of type [Phase](), can be {kTest, kValidation,
  kTrain}. It is used to filter this layer when creating the
  [NeuralNet]({{ BASE_PATH }}/docs/neural-net) for the excluding phase. E.g.,
  the "data" layer would be filtered when creating the NeuralNet instance for test phase.
  * `param`: configuration for a [Param]({{ BASE_PATH }}/docs/param) instance.
  There can be multiple Param objects in one layer.
  * `partition_dim`: integer value indicating the partition dimension of this
  layer. -1 (the default value) for no partitioning, 0 for partitioning on batch dimension, 1 for
  partitioning on feature dimension. It is used by
  [CreateGraph]({{ BASE_PATH }}/docs/neural-net) for partitioning the neural net.

Different layers may have different configurations, and these configurations are defined in `<type>_conf`, which is for a specific layer with type `<type>`, e.g., the "data" layer has `sharddata_conf` and "fc1" layer has `innerproduct_conf`. The specific configurations of built-in layers will be described in the [third section](#builtin) of this page.

<a name ="baselayer">
## Base Layer class

All layer implementations must subclass the [base layer class]().

### Members

    LayerProto layer_proto_;
    Blob<float> data_, grad_;
    vector<Layer*> srclayers_, dstlayers_;

The base layer class keeps the user configuration in `layer_proto_`. Source
layers and destination layers are stored in `srclayers_` and `dstlayers_`, respectively.
Almost all layers has $b$ (mini-batch size) feature vectors, which are stored
in the `data_` [Blob]() (A Blob is a chunk of memory space, proposed in [Caffe]).
There are layers without feature vectors; instead, they use other
layers' feature vectors. In this case, the `data_` field is not used.
The `grad_` Blob is for storing the gradients of the
objective loss w.r.t. the `data_` Blob. It is necessary in [BP algorithm],
hence we put it as a member of the base class. For [CD algorithm], the `grad_`
field is not used; instead, the layer from RBM may have a Blob for the positive
phase feature and a Blob for the negative phase feature. For a recurrent layer
in RNN, the feature blob contains one vector per internal layer.

If a layer has parameters, these parameters are declared using type
[Param](). Since some layers do not have
parameters, we do not declare any `Param` in the base layer class.

### Functions

    virtual void Setup(const LayerProto& proto, int npartitions = 1);
    virtual void ComputeFeature(Phase phase, Metric* perf) = 0;
    virtual void ComputeGradient(Phase phase) = 0;

The `Setup` function reads user configuration, i.e. `proto`, and information
from source layers, e.g., mini-batch size,  to set the
shape of the `data_` (and `grad_`) field as well
as some other layer specific fields. If `npartitions` is larger than 1, then
users need to reduce the sizes of `data_`, `grad_` Blobs or Param objects. For
example, if the `partition_dim=0` and there is no source layer, e.g., this
layer is a (bottom) data layer, then its `data_` and `grad_` Blob should have
`b/npartitions` feature vectors; If the source layer is also partitioned on
dimension 0, then this layer should have the same number of feature vectors as
the source layer. More complex partition cases are discussed in
[Neural net partitioning]({{ BASE_PATH }}/docs/neural-net). Typically, the
Setup function just set the shapes of `data_` Blobs and Param objects. Memory
will not be allocated until computation over the data structure happens.

The `ComputeFeature` function evaluates the feature blob by transforming (e.g.
convolution and pooling) features from the source layers.  `ComputeGradient`
computes the gradients of parameters associated with this layer.  These two
functions are invoked by the [TrainOneBatch]({{ BASE_PATH }}/docs/train-one-batch)
function during training. Hence, they should be consistent with the
`TrainOneBatch` function. Particularly, for feed-forward models and RNN models, they are
trained using BP algorithm, which requires each layer's `ComputeFeature`
function to compute `data_` based on source layers, and requires each layer's
`ComputeGradient` to compute gradients of parameters and source layers'
`grad_`. For energy models, e.g., RBM, they are trained by CD algorithm, which
requires each layer's `ComputeFeature` function to compute the feature vectors
for the positive phase or negative phase depending on the `phase` argument, and
requires the `ComputeGradient` function to only compute parameter gradients.
For some layers, e.g., loss layer or output layer, they can put the loss or
prediction result into the `metric` argument, which will be averaged and
displayed periodically.

<a name ="builtin">
## Built-in Layer subclasses

SINGA has provided many built-in layers, which can be used directly to create neural nets.
These layers are categorized as follows according to their functionalities,

  * Data layers for loading records (e.g., images) from disk, HDFS or network into memory.
  * Parser layers for parsing features, labels, etc. from records.
  * Neuron layers for feature transformation, e.g., convolution, pooling, dropout, etc.
  * Loss layers for measuring the training objective loss, e.g., cross entropy-loss or Euclidean loss.
  * Output layers for outputting the prediction results (e.g., probabilities of each category) onto disk or network.
  * Connection layers for connecting layers when the neural net is partitioned.

###Data Layers

Data layers load training/test data and convert them into [Record]()s, which
are parsed by parser layers. The data source can be disk file, HDFS, database
or network.

#### ShardData Layer

ShardData layer is used to read data from disk file. The file should be created
using [DataShard](). With the data file prepared, users configure the layer as

    type: kShardData
    sharddata_conf {
      path: "path to data shard folder"
      batchsize: int
      random_skip: int
    }

The batchsize specifies the number of records to be trained for one mini-batch.
The first rand() % `random_skip` Records will be skipped at the first
iteration. This is to enforce that different workers work on different Records.

#### LMDBData Layer
This data layer is similar to ShardDataLayer, except that the Records are
loaded from LMDB.

    type: kLMDBData
    lmdbdata_conf {
      path: "path to LMDB folder"
      batchsize: int
      random_skip: int
    }


###Parser Layers
Parser layers get a vector of Records from data layers and parse features into
a Blob.

    virtual void ParseRecords(Phase phase, const vector<Record>& records, Blob<float>* blob) = 0;


#### LabelLayer
Label layer is used to parse a single label from each Record. Consequently, it
will put $b$ (mini-batch size) values into the Blob. It has no specific configuration fields.


#### MnistImageLayer
This layer parses the pixel values of one image from the MNIST dataset. The pixel
values may be normalized as `x/norm_a - norm_b`. For example, if `norm_a` is
set to 255 and `norm_b` is set to 0, then every pixel will be normalized into
[0, 1].

    type: kMnistImage
    mnistimage_conf {
      norm_a: float
      norm_b: float
    }

#### RGBImageLayer
This layer parses the RGB values of one image from each Record. It may also
apply some transformations, e.g., cropping, mirroring operations. If the
`meanfile` is specified, it should point to a path that contains one Record for
the mean of each pixel over all training images.

    type: kRGBImage
    rgbimage_conf {
      scale: float
      cropsize: int
      mirror: bool
      meanfile: "Image_Mean_File_Path"
    }

### PrefetchLayer

This layer embed data layers and parser layers to do prefetch. It will launch a
thread to call the data layers and parser layers to load and extract features.
It ensures that the I/O task and computation/communication task can work simultaneously.
One example PrefetchLayer configuration is,

    layer {
      name: "prefetch"
      type: kPrefetch
      sublayers {
        name: "data"
        type: kShardData
        sharddata_conf { }
      }
      sublayers {
        name: "rgb"
        type: kRGBImage
        srclayers:"data"
        rgbimage_conf { }
      }
      sublayers {
        name: "label"
        type: kLabel
        srclayers: "data"
      }
      exclude:kTest
    }

The layers on top of the PrefetchLayer should use the name of the embedded
layers as their source layers. For example, the "rgb" and "label" should be
configured to the `srclayers` of other layers.

### Neuron Layers

Neuron layers conduct feature transformations.

####  ConvolutionLayer

Convolution layer is a basic layer used in constitutional neural net. It is used to extract local feature following some local patterns from slide windows in the image.

    layer
    {
    	name: "Conv_Number"
    	type: kConvolution
    	srclayers: "Src_Layer_Name"
    	convolution_conf
    	{
    		num_filters: int
    		kernel: int
    		stride: int
    		pad: int
    	}
    	param
    	{
    		name: "weight"
    		init_method:kGaussian|kConstant:kUniform|kPretrained|kGaussianSqrtFanIn|kUniformSqrtFanIn|kUniformSqrtFanInOut
    		/*use specific param of each init methods*/
    		learning_rate_multiplier:float
    	}
    	param
    	{
    		name: "bias"
    		init_method:kConstant|kGaussian|kUniform|kPretrained|kGaussianSqrtFanIn|kUniformSqrtFanIn|kUniformSqrtFanInOut
    		/**use specific param of each init methods**/
    		learning_rate_multiplier:float
    	}
    }

The int value num_filters stands for the count of the applied filters, the int value kernel stands for the convolution kernel size, the int value stride stands for the distance between the successive filters and the int value pad pads the images with a given int number of pixels border of zeros

The init_method has various optional value, each value stands for a specific method and they are described below:
* `kGaussian`: Sample Gaussian with std and mean
* `kUniform`: Uniform sampling between low and high
* `kPretrained`: From Toronto Convnet, let a=1/sqrt(fan_in), w*=a after generating from Gaussian distribution
* `kGaussianSqrtFanIn`: From Toronto Convnet, rectified linear activation, let a=sqrt(3)/sqrt(fan_in), and range is [-a,+a]. No need to set value=sqrt(3), the program will multiply it
* `kUniformSqrtFanIn`: From Theano MLP tutorial, let a=1/sqrt(fan_in+fan_out). For tanh activation, range is [-6a,+6a], and for sigmoid activation, range is [-24a,+24a]. Put the scale factor to value field  

Some params will be optional; For Constant Init, use value:float; For Gaussian Init, use mean:float, std:float; For Uniform Init, use low:float, high:float


####  InnerProduct Layer

InnerProduct Layer is a fully connected layer which is the basic element in feed forward neural network.
It uses the lower layer for an input vector V and outputs a vector H by doing the matrix-vector multiplication:
H = W*V + B where W and B are its weight and bias parameter, respectively.

    layer
    {
    	name: "IP_Number"
    	type: kInnerProduct
    	srclayers: "Src_Layer_Name"
    	innerproduct_conf
    	{
    		num_output: int   // The number of the filters
    	}
    	param
    	{
    		name: "weight"
    		init_method:kGaussian|kConstant:kUniform|kPretrained|kGaussianSqrtFanIn|kUniformSqrtFanIn|kUniformSqrtFanInOut
    		std: float
    		learning_rate_multiplier: float
    		weight_decay_multiplier: int
    		/*optional:low:float,high:float*/
    	}
    	param
    	{
    		name: "bias"
    		init_method:kConstant|kGaussian|kUniform|kPretrained|kGaussianSqrtFanIn|kUniformSqrtFanIn|kUniformSqrtFanInOut
    		learning_rate_mulitiplier: float
    		weight_decay_multiplier: int
    		value: int
    		/*optional:low:float,high:float*/
    	}
    }


####  Pooling Layer

The pooling layer is used to reduce the dimension of the feature vectors which are the output of the convolution layer. It may probably overcome the over-fitting.

    layer
    {
    	name: "Pool_Number"
    	type: kPooling
    	srclayers: "Src_Layer_Name"
    	pooling_conf
    	{
    		pool: AVE|MAX    // Choose whether use the Average Pooling or Max Pooling
    		kernel: int			 // size of the kernel filter
        pad: int         // the padding size
    		stride: int			 // the step length of the filter
    	}
    }

The pooling layer has two methods: Average Pooling and Max Pooling. Use the enum AVE and MAX to choose the method.
* Max Pooling uses a specific scaning window to find the max value
* Average Pooling scans all the values in the window to calculate the average value  


#### ReLU Layer

  The rectifier function is an activation function f(x) = Max(0, x) which can be used by neurons just like any other activation functions. A node using the rectifier activation function is called a ReLu node. The main reason that it is used is because of how efficiently it can be computed compared to more conventional activation functions like the sigmoid and hyperbolic tangent, without making a significant difference to generalisation accuracy. The rectifier activation function is used instead of a linear activation function to add non linearity to the network, otherwise the network would only ever be able to compute a linear function.

    layer
    {
    	name: "Relu_Number"
    	type: kReLU
    	srclayers: "Src_Layer_Name"
    }

#### Tanh Layer
Tanh uses the tanh as activation function. It transforms the input into range [-1, 1].

    layer
    {
    	name: "Tanh_Number"
    	type: kTanh
    	srclayer: "Src_Layer_Name"
    }

#### Dropout Layer
Dropout Layer is a layer that randomly dropouts some inputs. This scheme helps deep learning model away from over-fitting.

    layer
    {
    	name: "Dropout"
    	type: kDropout
    	dropout_conf
    	{
    		dropout_ratio: float
    	}
    	srclayers: "Src_Layer_Name"
    }

#### LRN Layer

Local Response Normalization normalizes over the local input areas. It provides two modes: WITHIN_CHANNEL and ACROSS_CHANNELS.

    layer
    {
    	name: "Norm_Number"
    	type: kLRN
    	lrn_conf
    	{
    		norm_region: WITHIN_CHANNEL|ACROSS_CHANNELS
    		local_size: int
    		alpha: float   //scaling parameter
    		beta: float    //exponential number
    	}
    	srclayers: "Src_Layer_Name"
    }

The param `local_size` has its specific use when choosing the different `norm_region`. For `WITHIN_CHANNEL`, it means the side length of the space region which will be summed up. For `ACROSS_CHANNELS`, it means the quantity of the adjoining channels which will be summed up.

### Loss Layers

#### SoftmaxLoss Layer
Softmax Loss Layer is the implementation of multi-class softmax loss function. It is generally used as the final layer to generate labels for classification tasks.

    layer
    {
    	name: "loss"
    	type: kSoftmaxLoss
    	softmaxloss_conf
    	{
    		topk: int
    	}
    	srclayers: "Src_Layer_Name1"
    	srclayers: "Src_Layer_Name2"
    }


### Other Layers

Util layers

#### Concate Layer

Concate Layer is used to concatenate the last dimension of the output of two nodes.

    layer
    {
    	name: "concate"
    	type: kConcate
    	concate_conf
    	{
    		concate_dim: int   // define the dimension
    	}
    	srclayers: "Src_Layer_Name"
    }


#### Slice Layer

The Slice layer slices an input layer to multiple output layers. A param slice_dim should be given which stands for the dimension.

    layer
    {
        name: "slice"
        type: kSlice
        slice_conf
        {
            slice_dim: int
        }
        srclayers: "Src_Layer_Name"
    }


#### Split Layer

The Split Layer can seperate the input blob into several output blobs. It is used to the situation which one input blob should be input to several other output blobs.

    layer
    {
        name: "split"
        type: kSplit
        split_conf
        {
            num_splits: int
        }
        srclayers: "Src_Layer_Name"
    }

#### BridgeSrc & BridgeDst Layer

BridgeSrc & BridgeDst Layer are utility layers implementing logics of model partition. It can be used as a lock for synchronization, a transformation storage of different type of model partition and etc.

<a name ="newlayer">
## Implementing a new Layer subclass

Users can extend the base layer class to implement their own feature transformation
logics as long as the two virtual functions are overridden to be consistent with
the `TrainOneBatch` function. The `Setup` function may also be overridden to
read specific layer configuration.


### Layer specific protocol message

To implement a new layer, the first step is to define the layer specific
configuration. Suppose the new layer is of FooLayer, the layer specific
google protocol message `FooLayerProto` should be defined as

    # in user.proto
    package singa
    import "job.proto"
    message FooLayerProto {
      optional int32 a = 1;  // specific fields to the FooLayer
    }

In addition, users need to extend the original `LayerProto` (defined in job.proto of SINGA)
to include the `foo_conf` as follows.

    extend LayerProto {
      optional FooLayerProto foo_conf = 201;  // unique field id, reserved for extensions
    }

If there are multiple new layers, then each layer that has specific
configurations would have a `<type>_conf` field and takes one unique extension number.
SINGA has reserved enough extension numbers, e.g., starting from 201 to 1000.

    # job.proto of SINGA
    LayerProto {
      ...
      extensions 201 to 1000;
    }

With user.proto defined, users can use [protoc][1] to generate the `user.pb.cc`
and `user.pb.h` files.  In users' code, the extension fields can be accessed via,

    auto conf = layer_proto_.GetExtension(foo_conf);
    int a = conf.a();

When defining configurations of the new layer (in job.conf), users should use `user_type` for its layer type instead of `type`. In addition, `foo_conf` should be enclosed in brackets.

    layer {
      name: "foo"
      user_type: "kFooLayer"  # Note user_type of user-defined layers is string
      [singa.foo_conf] {      # Note there is a pair of [] for extension fields
        a: 10
      }
    }

### New Layer subclass declaration

The new layer subclass can be implemented following the built-in layer subclasses.

    class FooLayer : public Layer {
     public:
      void Setup(const LayerProto& proto, int npartitions = 1) override;
      void ComputeFeature(Phase phase, Metric* perf) override;
      void ComputeGradient(Phase phase) override;

     private:
      //  members
    };

Users must override the two virtual functions to be called by the
`TrainOneBatch` for either BP or CD algorithm. Typically, the `Setup` function
will also be overridden to initialize some members. The user configured fields
can be accessed through `layer_proto_` as shown in the above paragraphs.

### New Layer subclass registration

The newly defined layer should be registered in [driver.cc]({{ BASE_PATH }}/docs/programming-guide) by

    RegisterLayer<FooLayer>("kFooLayer"); // "kFooLayer" should be matched to layer configurations in job.conf.

After that, the
[NeuralNet]({{ BASE_PATH }}/docs/neural-net) can create instances of the new Layer subclass.

[1]: https://developers.google.com/protocol-buffers/docs/cpptutorial#compiling-your-protocol-buffers
